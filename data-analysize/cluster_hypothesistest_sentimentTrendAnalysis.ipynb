{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba079f6",
   "metadata": {},
   "source": [
    "# 死刑舆论情感分析和倾向分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c8faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置图片清晰度\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# 正常显示中文，设置字体为宋体\n",
    "plt.rcParams['font.sans-serif'] = ['SimSun']\n",
    "# 加载数据\n",
    "df = pd.read_csv(\"D:\\\\HuaweiMoveData\\\\Users\\\\32549\\\\OneDrive\\\\twitter_capital_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 TextBlob 进行情感分析\n",
    "def get_sentiment_score(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# 应用函数到 description 列，获取情感得分\n",
    "df['sentiment_score'] = df['description'].apply(get_sentiment_score)\n",
    "\n",
    "# 定义一个函数来对情感得分进行分类\n",
    "def get_sentiment_label(score):\n",
    "    if score > 0.05:\n",
    "        return 'Positive'\n",
    "    elif score < -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# 应用函数到 sentiment_score 列，获取情感标签\n",
    "df['sentiment_label'] = df['sentiment_score'].apply(get_sentiment_label)\n",
    "\n",
    "# 查看总体情感趋向分布情况\n",
    "sentiment_distribution = df['sentiment_label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# 输出结果（保留两位小数）\n",
    "print(sentiment_distribution.round(2))\n",
    "\n",
    "\n",
    "\n",
    "# 绘制情感得分直方图\n",
    "plt.figure()\n",
    "plt.hist(df['sentiment_score'], bins=30, edgecolor='black')\n",
    "plt.title('情感得分分布直方图')\n",
    "plt.xlabel('情感得分')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('频数')\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420babde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 datetime 列转换为日期时间类型\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# 按年月统计不同情感的数量\n",
    "time_series_data = df.groupby([df['datetime'].dt.to_period('M'), 'sentiment_label']).size().unstack(fill_value=0)\n",
    "\n",
    "# 绘制时间序列图\n",
    "\n",
    "\n",
    "ax = time_series_data.plot(title='死刑舆情随时间的变化趋势')\n",
    "ax.set_xlabel('时间')\n",
    "ax.set_ylabel('数量')\n",
    "ax.legend(title='情感倾向')\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807cf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"D:\\\\HuaweiMoveData\\\\Users\\\\32549\\\\OneDrive\\\\twitter_capital_data.csv\")\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "df[\"month_str\"] = df[\"datetime\"].dt.strftime(\"%Y-%m\")\n",
    "df[\"description\"] = df[\"description\"].fillna(\"\").str.lower()\n",
    "\n",
    "# 按月合并文本\n",
    "monthly_texts = df.groupby(\"month_str\")[\"description\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "\n",
    "# 提取关键词频率\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", max_features=20)\n",
    "monthly_keywords = []\n",
    "\n",
    "\n",
    "for text in monthly_texts[\"description\"]:\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    monthly_keywords.append(dict(zip(vectorizer.get_feature_names_out(), X.toarray()[0])))\n",
    "\n",
    "print(monthly_keywords)\n",
    "\n",
    "# 构建 DataFrame（行为月份，列为关键词）\n",
    "keywords_df = pd.DataFrame(monthly_keywords)\n",
    "keywords_df[\"month\"] = monthly_texts[\"month_str\"]\n",
    "keywords_df.set_index(\"month\", inplace=True)\n",
    "keywords_df.fillna(0, inplace=True)\n",
    "\n",
    "# 画堆积图\n",
    "keywords_df.plot(kind=\"bar\", stacked=True, figsize=(16, 8), colormap=\"tab20\")\n",
    "plt.title(\"每月死刑话题关键词频率变化图\")\n",
    "plt.xlabel(\"月份\")\n",
    "plt.ylabel(\"词频\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:\\\\HuaweiMoveData\\\\Users\\\\32549\\\\OneDrive\\\\twitter_capital_data.csv\")\n",
    "df[\"description\"] = df[\"description\"].fillna(\"\").str.lower()\n",
    "\n",
    "# 可选：你自己标注 is_sensational 为 True 的行，或基于关键词筛选\n",
    "keywords = [\"execution\", \"innocent\", \"appeal\", \"cop\", \"rape\", \"mass shooting\", \n",
    "            \"black\", \"racial\", \"jury\", \"wrongfully\", \"death row\", \"victim\", \"kill\", \"child\", \"judge\"]\n",
    "\n",
    "df[\"is_sensational\"] = df[\"description\"].apply(lambda x: any(k in x for k in keywords))\n",
    "\n",
    "# 提取高关注文本\n",
    "texts = df[df[\"is_sensational\"] == True][\"description\"].dropna()\n",
    "full_text = \" \".join(texts)\n",
    "\n",
    "# 提取词频\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=100)\n",
    "X = vectorizer.fit_transform([full_text])\n",
    "word_freq = dict(zip(vectorizer.get_feature_names_out(), X.toarray()[0]))\n",
    "print(word)\n",
    "\n",
    "# 生成词云\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='inferno')\n",
    "wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"死刑重大事件关键词词云\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 展示每月的数据\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "#\n",
    "df = pd.read_csv(\"D:\\\\HuaweiMoveData\\\\Users\\\\32549\\\\OneDrive\\\\twitter_capital_data.csv\")\n",
    "# 自定义停用词列表\n",
    "stop_words = set([\n",
    "    'the', 'and', 'to', 'of', 'a', 'in', 'that', 'it', 'with', 'as', 'for', 'on',\n",
    "    'is', 'are', 'be', 'was', 'were', 'by', 'at', 'this', 'from', 'or', 'an', 'have',\n",
    "    'not', 'but', 'has', 'had', 'its', 'their', 'they', 'we', 'you', 'i', 'he', 'she',\n",
    "    'his', 'her', 'him', 'our', 'your', 'all', 'any', 'no', 'will', 'would', 'can',\n",
    "    'could', 'may', 'might', 'should', 'these', 'those', 'am', 'been', 'being', 'do',\n",
    "    'does', 'did', 'so', 'such', 'than', 'then', 'there', 'here', 'when', 'where',\n",
    "    'why', 'how', 'what', 'which', 'who', 'whom', 'into', 'about', 'between', 'through',\n",
    "    'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "    'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n",
    "    'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    "    'such', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 'very','s', 't', 'just',\n",
    "    'don', 'should', 'now', 'd', 'll','m', 'o','re', 've', 'y', 'ain', 'aren', 'couldn',\n",
    "    'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn','ma','mightn','mustn', 'needn',\n",
    "    'shan','shouldn', 'wasn', 'weren', 'won', 'wouldn'\n",
    "])\n",
    "\n",
    "# 文本预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    # 移除特殊字符和数字\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    # 分词并去除停用词\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    # 简单词干化（去除复数形式）\n",
    "    tokens = [word.rstrip('s') for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# 合并 description 和 title 列的文本\n",
    "combined_text = df['description'] +' '+ df['title']\n",
    "\n",
    "# 将日期列转换为 datetime 类型\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# 按月份分组处理数据\n",
    "for month, group in df.groupby(df['datetime'].dt.to_period('M')):\n",
    "    print(f\"月份: {month}\")\n",
    "\n",
    "    # 对当前月份的组合文本进行预处理\n",
    "    processed_text = group['description'].fillna('') +' '+ group['title'].fillna('')\n",
    "    processed_text = processed_text.apply(preprocess_text)\n",
    "\n",
    "    # 将处理后的文本转换回字符串（用于 TfidfVectorizer）\n",
    "    corpus = [' '.join(tokens) for tokens in processed_text]\n",
    "\n",
    "    # 与死刑话题相关的词汇列表\n",
    "    death_penalty_terms = ['death', 'penalty', 'capital', 'punishment', 'execute', 'abolish', 'convict','sentence']\n",
    "\n",
    "    # 统计词频\n",
    "    all_tokens = [token for sublist in processed_text for token in sublist]\n",
    "    word_freq = Counter(all_tokens)\n",
    "\n",
    "    # 筛选出与死刑话题相关的词频\n",
    "    death_penalty_freq = {term: freq for term, freq in word_freq.items()\n",
    "                          if any(t in term for t in death_penalty_terms)}\n",
    "\n",
    "    # 按词频降序排序\n",
    "    sorted_death_penalty_freq = sorted(death_penalty_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 输出死刑相关词频结果\n",
    "    print(\"与死刑话题相关的词频统计（降序）：\")\n",
    "    for term, freq in sorted_death_penalty_freq[:20]:  # 只显示前 20 个高频词\n",
    "        print(f\"{term}: {freq}\")\n",
    "\n",
    "    # 使用 TF - IDF 向量化\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=0.95,  # 忽略在超过 95% 文档中出现的词\n",
    "        min_df=2,  # 忽略在少于 2 篇文档中出现的词\n",
    "        max_features=1000,  # 保留最高频的 1000 个词\n",
    "        stop_words='english'  # 使用 sklearn 内置的英文停用词\n",
    "    )\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # 获取特征名称（词汇表）\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # 训练 LDA 模型进行主题建模\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=5,  # 主题数量\n",
    "        max_iter=10,  # 最大迭代次数\n",
    "        learning_method='online',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    lda_model = lda.fit(tfidf_matrix)\n",
    "\n",
    "    # 输出每个主题的前 10 个关键词\n",
    "    print(\"\\n抽取的中心话题（每个主题前 10 个关键词）：\")\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_words_idx = topic.argsort()[-10:][::-1]  # 获取前 10 个关键词的索引\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd652d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 使用前面代码中的df和情感得分\n",
    "\n",
    "# 定义关键事件\n",
    "events = [\n",
    "    {'month': '2017-07', 'event': '死刑执行案例增加', 'sentiment': 'positive'},\n",
    "    {'month': '2017-08', 'event': '涉毒死刑案件讨论', 'sentiment': 'positive'},\n",
    "    {'month': '2017-09', 'event': '争议点出现', 'sentiment': 'neutral'},\n",
    "    {'month': '2017-10', 'event': '恐怖犯罪死刑案件', 'sentiment': 'positive'},\n",
    "    {'month': '2017-10', 'event': '废除死刑讨论升温', 'sentiment': 'negative'}\n",
    "]\n",
    "\n",
    "# 绘制情感趋势图\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df.index, df['neutral_score'], 'o-', label='中性情感', color='gray', linewidth=2)\n",
    "plt.plot(df.index, df['positive_score'], '^-', label='正向情感', color='green', linewidth=2)\n",
    "plt.plot(df.index, df['negative_score'], 's-', label='负向情感', color='red', linewidth=2)\n",
    "\n",
    "# 添加事件标记\n",
    "for event in events:\n",
    "    month_idx = list(df.index).index(event['month'])\n",
    "    if event['sentiment'] == 'positive':\n",
    "        score = df['positive_score'][month_idx]\n",
    "        plt.annotate(event['event'], xy=(month_idx, score), xytext=(month_idx, score + 500),\n",
    "                     arrowprops=dict(facecolor='green', shrink=0.05), color='green', fontweight='bold')\n",
    "    elif event['sentiment'] == 'negative':\n",
    "        score = df['negative_score'][month_idx]\n",
    "        plt.annotate(event['event'], xy=(month_idx, score), xytext=(month_idx, score + 500),\n",
    "                     arrowprops=dict(facecolor='red', shrink=0.05), color='red', fontweight='bold')\n",
    "    else:\n",
    "        score = df['neutral_score'][month_idx]\n",
    "        plt.annotate(event['event'], xy=(month_idx, score), xytext=(month_idx, score + 500),\n",
    "                     arrowprops=dict(facecolor='gray', shrink=0.05), color='gray', fontweight='bold')\n",
    "\n",
    "plt.title('2017年6-10月死刑话题情感倾向与关键事件关联')\n",
    "plt.xlabel('月份')\n",
    "plt.ylabel('关键词词频总和')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f364a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1609055",
   "metadata": {},
   "source": [
    "# 美国死刑记录假设检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fbd1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib\n",
    "\n",
    "# 设置全局字体为宋体（SimSun）\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimSun']  # 中文宋体\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False    # 解决负号显示问题\n",
    "\n",
    "# 读取数据\n",
    "file_path = \"D:\\\\HuaweiMoveData\\\\Users\\\\32549\\\\OneDrive\\\\executions-to-2002 (2).csv\"  # 替换为你的文件路径\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 选取相关字段并清洗缺失值\n",
    "df_clean = df[['race', 'SexOfOffender', 'STATE OF CONVICTION']].dropna()\n",
    "\n",
    "# 清洗字段内容：提取括号后的真实标签（部分数据格式为 \"(代码) Label\"）\n",
    "df_clean['race'] = df_clean['race'].str.extract(r'\\)\\s*(.*)$')[0].fillna(df_clean['race'])\n",
    "df_clean['SexOfOffender'] = df_clean['SexOfOffender'].str.extract(r'\\)\\s*(.*)$')[0].fillna(df_clean['SexOfOffender'])\n",
    "df_clean['STATE OF CONVICTION'] = df_clean['STATE OF CONVICTION'].str.extract(r'\\)\\s*(.*)$')[0].fillna(df_clean['STATE OF CONVICTION'])\n",
    "\n",
    "# ====== 可视化每个变量的分布 ======\n",
    "race_counts = df_clean['race'].value_counts()\n",
    "sex_counts = df_clean['SexOfOffender'].value_counts()\n",
    "state_counts = df_clean['STATE OF CONVICTION'].value_counts().head(10)\n",
    "# 马卡龙配色\n",
    "custom_colors = [\n",
    "    \"#a1c6ea\",  # 柔蓝\n",
    "    \"#fcbad3\",  # 烟粉\n",
    "    \"#ffdac1\",  # 奶杏\n",
    "    \"#b5ead7\",  # 薄荷绿\n",
    "    \"#c7ceea\",  # 淡紫\n",
    "    \"#ffb5a7\",  # 温柔珊瑚\n",
    "    \"#b8bedd\",  # 烟蓝紫\n",
    "    \"#f7b7a3\",  # 桃粉橘\n",
    "]\n",
    "\n",
    "# 可视化每个变量的分布\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x=race_counts.values, y=race_counts.index, palette=custom_colors)\n",
    "plt.title('种族分布')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x=sex_counts.values, y=sex_counts.index, palette=custom_colors)\n",
    "plt.title('性别分布')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x=state_counts.values, y=state_counts.index, palette=custom_colors)\n",
    "plt.title('定罪州前10名分布')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ====== 卡方检验分析 ======\n",
    "\n",
    "# 1. 种族与性别的关系\n",
    "race_sex_table = pd.crosstab(df_clean['race'], df_clean['SexOfOffender'])\n",
    "chi2_race_sex, p_race_sex, _, _ = chi2_contingency(race_sex_table)\n",
    "\n",
    "# 2. 种族与定罪州的关系\n",
    "race_state_table = pd.crosstab(df_clean['race'], df_clean['STATE OF CONVICTION'])\n",
    "chi2_race_state, p_race_state, _, _ = chi2_contingency(race_state_table)\n",
    "\n",
    "# 3. 性别与定罪州的关系\n",
    "sex_state_table = pd.crosstab(df_clean['SexOfOffender'], df_clean['STATE OF CONVICTION'])\n",
    "chi2_sex_state, p_sex_state, _, _ = chi2_contingency(sex_state_table)\n",
    "\n",
    "# 输出结果\n",
    "print(\"卡方检验 p 值：\")\n",
    "print(f\"1. 种族 与 性别：{p_race_sex:.4f}\")\n",
    "print(f\"2. 种族 与 定罪州：{p_race_state:.4e}\")\n",
    "print(f\"3. 性别 与 定罪州：{p_sex_state:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d847600d",
   "metadata": {},
   "source": [
    "# 聚类分析——正负两类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98990954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5321050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 文件读取地址\n",
    "file_path = r\"D:\\HuaweiMoveData\\Users\\32549\\OneDrive\\大二下\\数据科学与数据分析\\小组作业\\死刑和遗言数据.csv\"\n",
    "\n",
    "# 加载数据\n",
    "try:\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    data = pd.read_csv(file_path, encoding='gbk')\n",
    "\n",
    "# 去除缺失值\n",
    "data = data.dropna(subset=['last statement'])\n",
    "\n",
    "# 加载停用词\n",
    "with open(\"D:\\\\Users\\\\32549\\\\PycharmProjects\\\\数据科学聚类\\\\stopwords.txt\", 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# 更完善的文本预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 去除特殊字符\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # 分词\n",
    "    words = jieba.lcut(text)\n",
    "    # 词性标注，保留形容词、副词等可能体现情感的词性\n",
    "    import jieba.posseg as pseg\n",
    "    words = [word for word, flag in pseg.lcut(text) if flag in ['a', 'ad', 'd'] or word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 对遗言进行预处理\n",
    "data['last statement_preprocessed'] = data['last statement'].apply(preprocess_text)\n",
    "\n",
    "# 使用 TF-IDF 进行文本向量化\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(data['last statement_preprocessed'])\n",
    "\n",
    "# 增加情感分析特征\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "data['sentiment_score'] = data['last statement_preprocessed'].apply(get_sentiment)\n",
    "X_sentiment = data['sentiment_score'].values.reshape(-1, 1)\n",
    "\n",
    "# 合并特征\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([X_tfidf, X_sentiment])\n",
    "\n",
    "# 寻找最优聚类数\n",
    "silhouette_scores = []\n",
    "for n_cluster in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=n_cluster, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "best_n_cluster = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "\n",
    "# 使用最优聚类数进行 K-Means 聚类\n",
    "kmeans = KMeans(n_clusters=best_n_cluster, random_state=42)\n",
    "data['cluster_label'] = kmeans.fit_predict(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=best_n_cluster, random_state=42)\n",
    "kmeans.fit(X)  # 训练全部数据\n",
    "data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# 分析每个聚类的特征，结合其他信息构建用户画像\n",
    "for cluster in range(best_n_cluster):\n",
    "    cluster_data = data[data['cluster_label'] == cluster]\n",
    "    print(f\"聚类 {cluster} 的用户画像：\")\n",
    "    print(f\"平均年龄：{cluster_data['age'].mean():.2f}\")\n",
    "    print(f\"主要种族：{cluster_data['race'].value_counts().idxmax()}\")\n",
    "    print(f\"主要性别：{cluster_data['gender'].value_counts().idxmax()}\")\n",
    "    print(f\"主要教育程度：{cluster_data['education level'].value_counts().idxmax()}\")\n",
    "    print(f\"主要职业：{cluster_data['prior occupation'].value_counts().idxmax()}\")\n",
    "    print(f\"有监狱记录比例：{cluster_data['prior prison record(0/1)'].mean():.2%}\")\n",
    "    print(f\"平均情感得分：{cluster_data['sentiment_score'].mean():.2f}\")\n",
    "    print(\"典型遗言示例：\")\n",
    "    print(cluster_data['last statement'].iloc[0])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加模型性能\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# 文件读取地址\n",
    "file_path = r\"D:\\HuaweiMoveData\\Users\\32549\\OneDrive\\大二下\\数据科学与数据分析\\小组作业\\死刑和遗言数据.csv\"\n",
    "\n",
    "# 加载数据\n",
    "try:\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    data = pd.read_csv(file_path, encoding='gbk')\n",
    "\n",
    "# 去除缺失值\n",
    "data = data.dropna(subset=['last statement'])\n",
    "\n",
    "# 加载停用词\n",
    "with open(\"D:\\\\Users\\\\32549\\\\PycharmProjects\\\\数据科学聚类\\\\stopwords.txt\", 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# 更完善的文本预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 去除特殊字符\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # 词性标注，保留形容词、副词等可能体现情感的词性\n",
    "    words = [word for word, flag in pseg.lcut(text) if flag in ['a', 'ad', 'd'] or word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 对遗言进行预处理\n",
    "data['last statement_preprocessed'] = data['last statement'].apply(preprocess_text)\n",
    "\n",
    "# 使用 TF-IDF 进行文本向量化\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(data['last statement_preprocessed'])\n",
    "\n",
    "# 增加情感分析特征\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "data['sentiment_score'] = data['last statement_preprocessed'].apply(get_sentiment)\n",
    "X_sentiment = data['sentiment_score'].values.reshape(-1, 1)\n",
    "\n",
    "# 合并特征\n",
    "X = hstack([X_tfidf, X_sentiment])\n",
    "\n",
    "# 寻找最优聚类数\n",
    "silhouette_scores = []\n",
    "for n_cluster in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=n_cluster, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "best_n_cluster = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "print(f\"最优聚类数: {best_n_cluster}\")\n",
    "\n",
    "# 使用最优聚类数进行 K-Means 聚类（对全部数据）\n",
    "kmeans = KMeans(n_clusters=best_n_cluster, random_state=42)\n",
    "kmeans.fit(X)\n",
    "data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# --- 新增：训练集/测试集划分及性能输出 ---\n",
    "X_train, X_test = train_test_split(X, test_size=0.3, random_state=42)\n",
    "\n",
    "kmeans_train_test = KMeans(n_clusters=best_n_cluster, random_state=42)\n",
    "kmeans_train_test.fit(X_train)\n",
    "\n",
    "train_labels = kmeans_train_test.labels_\n",
    "test_labels = kmeans_train_test.predict(X_test)\n",
    "\n",
    "train_silhouette = silhouette_score(X_train, train_labels)\n",
    "test_silhouette = silhouette_score(X_test, test_labels)\n",
    "\n",
    "print(f\"训练集轮廓系数: {train_silhouette:.4f}\")\n",
    "print(f\"测试集轮廓系数: {test_silhouette:.4f}\")\n",
    "\n",
    "# 分析每个聚类的特征，结合其他信息构建用户画像\n",
    "for cluster in range(best_n_cluster):\n",
    "    cluster_data = data[data['cluster_label'] == cluster]\n",
    "    print(f\"聚类 {cluster} 的用户画像：\")\n",
    "    print(f\"平均年龄：{cluster_data['age'].mean():.2f}\")\n",
    "    print(f\"主要种族：{cluster_data['race'].value_counts().idxmax()}\")\n",
    "    print(f\"主要性别：{cluster_data['gender'].value_counts().idxmax()}\")\n",
    "    print(f\"主要教育程度：{cluster_data['education level'].value_counts().idxmax()}\")\n",
    "    print(f\"主要职业：{cluster_data['prior occupation'].value_counts().idxmax()}\")\n",
    "    print(f\"有监狱记录比例：{cluster_data['prior prison record(0/1)'].mean():.2%}\")\n",
    "    print(f\"平均情感得分：{cluster_data['sentiment_score'].mean():.2f}\")\n",
    "    print(\"典型遗言示例：\")\n",
    "    print(cluster_data['last statement'].iloc[0])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "# 可视化1: 聚类结果的二维可视化 (PCA降维)\n",
    "def plot_clusters_pca(X, labels, n_clusters):\n",
    "    # 使用PCA降维到2D\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X.toarray() if hasattr(X, 'toarray') else X)\n",
    "    \n",
    "    # 创建散点图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, \n",
    "                          cmap='viridis', alpha=0.7, s=50)\n",
    "    \n",
    "    # 添加图例\n",
    "    legend = plt.legend(*scatter.legend_elements(),\n",
    "                       title=\"聚类\",\n",
    "                       loc=\"upper right\")\n",
    "    plt.gca().add_artist(legend)\n",
    "    \n",
    "    # 添加聚类中心\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "    plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "                c='red', marker='X', s=200, label='聚类中心')\n",
    "   \n",
    "    plt.title(f'K-means聚类结果 (PCA降维)')\n",
    "    plt.xlabel('主成分1')\n",
    "    plt.ylabel('主成分2')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('clusters_pca.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 可视化2: 聚类中心词云\n",
    "def plot_cluster_wordclouds(data, n_clusters, vectorizer):\n",
    "    # 获取特征名称\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # 为每个聚类生成词云\n",
    "    fig, axes = plt.subplots(1, n_clusters, figsize=(5*n_clusters, 5))\n",
    "    if n_clusters == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        # 获取该聚类的TF-IDF权重\n",
    "        cluster_indices = np.where(data['cluster_label'] == cluster)[0]\n",
    "        cluster_tfidf = X_tfidf[cluster_indices].toarray().sum(axis=0)\n",
    "        \n",
    "        # 创建词-权重字典\n",
    "        word_weights = {feature_names[i]: cluster_tfidf[i] \n",
    "                       for i in range(len(feature_names)) \n",
    "                       if cluster_tfidf[i] > 0}\n",
    "        \n",
    "        # 生成词云\n",
    "        wordcloud = WordCloud(width=400, height=400, \n",
    "                             background_color='white',\n",
    "                             max_words=100,\n",
    "                             contour_width=3,\n",
    "                             contour_color='steelblue')\n",
    "        wordcloud.generate_from_frequencies(word_weights)\n",
    "        \n",
    "        # 显示词云\n",
    "        axes[cluster].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[cluster].set_title(f'聚类 {cluster} 的关键词')\n",
    "        axes[cluster].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cluster_wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 可视化3: 情感分析可视化\n",
    "def plot_sentiment_analysis(data, n_clusters):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 箱线图\n",
    "    sns.boxplot(x='cluster_label', y='sentiment_score', data=data, palette='viridis')\n",
    "    plt.title('各聚类的情感得分分布')\n",
    "    plt.xlabel('聚类')\n",
    "    plt.ylabel('情感得分')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 直方图\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for cluster in range(n_clusters):\n",
    "        sns.histplot(data[data['cluster_label'] == cluster]['sentiment_score'], \n",
    "                    kde=True, label=f'聚类 {cluster}', alpha=0.5)\n",
    "    plt.title('各聚类的情感得分分布直方图')\n",
    "    plt.xlabel('情感得分')\n",
    "    plt.ylabel('频率')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_histogram.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 执行可视化\n",
    "plot_clusters_pca(X, data['cluster_label'], best_n_cluster)\n",
    "plot_cluster_wordclouds(data, best_n_cluster, vectorizer)\n",
    "plot_cluster_statistics(data, best_n_cluster)\n",
    "plot_sentiment_analysis(data, best_n_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ec13e",
   "metadata": {},
   "source": [
    "# 聚类分析1——使用bert模型正负两类 更优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11140e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在使用BERT进行文本向量化...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:12<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "聚类数 2, 轮廓系数: 0.1700\n",
      "聚类数 3, 轮廓系数: 0.1376\n",
      "聚类数 4, 轮廓系数: 0.1046\n",
      "聚类数 5, 轮廓系数: 0.1037\n",
      "聚类数 6, 轮廓系数: 0.0732\n",
      "聚类数 7, 轮廓系数: 0.0684\n",
      "聚类数 8, 轮廓系数: 0.0907\n",
      "聚类数 9, 轮廓系数: 0.0886\n",
      "最优聚类数: 2\n",
      "\n",
      "聚类 0 的用户画像：\n",
      "样本数量：229\n",
      "平均年龄：39.28\n",
      "主要种族：White\n",
      "主要性别：male\n",
      "主要教育程度：11\n",
      "主要职业：laborer\n",
      "有监狱记录比例：53.28%\n",
      "平均情感得分：0.20\n",
      "代表性关键词： family, im, know, like, love, sorry, thank, want, would, yes\n",
      "典型遗言示例：\n",
      "  1. Yes Warden, I would like to tell the family of the victim that I could never figure out the words to...\n",
      "  2. Yes ma’am, I want to thank y’all. I love y’all for supporting me. I want to apologize for the wrong ...\n",
      "  3. Yes, I just want to thank (pause) I don’t want to leave you baby, see you when you get there. I love...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "聚类 1 的用户画像：\n",
      "样本数量：224\n",
      "平均年龄：40.00\n",
      "主要种族：White\n",
      "主要性别：male\n",
      "主要教育程度：12\n",
      "主要职业：laborer\n",
      "有监狱记录比例：49.55%\n",
      "平均情感得分：0.16\n",
      "代表性关键词： family, god, know, like, love, sorry, thank, want, would, yall\n",
      "典型遗言示例：\n",
      "  1. There is not a day that goes by that I don't regret my actions, I had no right to take your loved on...\n",
      "  2. I will always love you no matter, that our love is uncontrollable there is no definition and no fell...\n",
      "  3. Yes ma'am, first I would like to apologize for all the wrong I have done, and for pain I've caused t...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c97b0f3",
   "metadata": {},
   "source": [
    "# 聚类分析——正向分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# 文件读取地址\n",
    "file_path = r\"D:\\HuaweiMoveData\\Users\\32549\\OneDrive\\大二下\\数据科学与数据分析\\小组作业\\死刑和遗言数据.csv\"\n",
    "\n",
    "# 加载数据\n",
    "try:\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    data = pd.read_csv(file_path, encoding='gbk')\n",
    "\n",
    "# 去除缺失值\n",
    "data = data.dropna(subset=['last statement'])\n",
    "\n",
    "# 文本预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 去除特殊字符\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # 分词\n",
    "    words = jieba.lcut(text)\n",
    "    # 假设没有停用词表，这里简单去除单个字符的词\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 对遗言进行预处理\n",
    "data['last statement_preprocessed'] = data['last statement'].apply(preprocess_text)\n",
    "\n",
    "# TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['last statement_preprocessed'])\n",
    "\n",
    "# 划分训练集和测试集（用于验证聚类性能）\n",
    "X_train, X_test, data_train, data_test = train_test_split(X, data, test_size=0.3, random_state=42)\n",
    "\n",
    "# 聚类数（可根据实际情况调优）\n",
    "num_clusters = 5\n",
    "\n",
    "# 训练集上训练 K-Means\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "train_labels = kmeans.fit_predict(X_train)\n",
    "\n",
    "# 测试集聚类标签（预测）\n",
    "test_labels = kmeans.predict(X_test)\n",
    "\n",
    "# 计算轮廓系数\n",
    "train_silhouette = silhouette_score(X_train, train_labels)\n",
    "test_silhouette = silhouette_score(X_test, test_labels)\n",
    "\n",
    "print(f\"训练集轮廓系数: {train_silhouette:.4f}\")\n",
    "print(f\"测试集轮廓系数: {test_silhouette:.4f}\")\n",
    "\n",
    "# 对所有数据重新打标签用于聚类画像分析\n",
    "data['cluster_label'] = kmeans.predict(X)\n",
    "\n",
    "# 分析每个聚类的特征，结合其他信息构建用户画像\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = data[data['cluster_label'] == cluster]\n",
    "    print(f\"\\n聚类 {cluster} 的用户画像：\")\n",
    "    print(f\"平均年龄：{cluster_data['age'].mean():.2f}\")\n",
    "    print(f\"主要种族：{cluster_data['race'].value_counts().idxmax()}\")\n",
    "    print(f\"主要性别：{cluster_data['gender'].value_counts().idxmax()}\")\n",
    "    print(f\"主要教育程度：{cluster_data['education level'].value_counts().idxmax()}\")\n",
    "    print(f\"主要职业：{cluster_data['prior occupation'].value_counts().idxmax()}\")\n",
    "    print(f\"有监狱记录比例：{cluster_data['prior prison record(0/1)'].mean():.2%}\")\n",
    "    print(\"典型遗言示例：\")\n",
    "    print(cluster_data['last statement'].iloc[0])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化1: 聚类结果的二维可视化 (PCA降维)\n",
    "def plot_clusters_pca(X, labels, n_clusters):\n",
    "    # 使用PCA降维到2D\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X.toarray())\n",
    "    \n",
    "    # 创建散点图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, \n",
    "                          cmap='viridis', alpha=0.7, s=50)\n",
    "    \n",
    "    # 添加图例\n",
    "    legend = plt.legend(*scatter.legend_elements(),\n",
    "                       title=\"聚类\",\n",
    "                       loc=\"upper right\")\n",
    "    plt.gca().add_artist(legend)\n",
    "    \n",
    "    # 添加聚类中心\n",
    "    centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "    plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "                c='red', marker='X', s=200, label='聚类中心')\n",
    "  \n",
    "    plt.title(f'K-means聚类结果 (PCA降维)')\n",
    "    plt.xlabel('主成分1')\n",
    "    plt.ylabel('主成分2')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('clusters_pca.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 可视化2: 聚类中心词云\n",
    "def plot_cluster_wordclouds(data, n_clusters, vectorizer):\n",
    "    # 获取特征名称\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # 为每个聚类生成词云\n",
    "    fig, axes = plt.subplots(1, n_clusters, figsize=(5*n_clusters, 5))\n",
    "    if n_clusters == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        # 获取该聚类的TF-IDF权重\n",
    "        cluster_indices = np.where(data['cluster_label'] == cluster)[0]\n",
    "        cluster_tfidf = X[cluster_indices].toarray().sum(axis=0)\n",
    "        \n",
    "        # 创建词-权重字典\n",
    "        word_weights = {feature_names[i]: cluster_tfidf[i] \n",
    "                       for i in range(len(feature_names)) \n",
    "                       if cluster_tfidf[i] > 0}\n",
    "        \n",
    "        # 生成词云\n",
    "        wordcloud = WordCloud(width=400, height=400, \n",
    "                             background_color='white',\n",
    "                             max_words=100,\n",
    "                             contour_width=3,\n",
    "                             contour_color='steelblue')\n",
    "        wordcloud.generate_from_frequencies(word_weights)\n",
    "        \n",
    "        # 显示词云\n",
    "        axes[cluster].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[cluster].set_title(f'聚类 {cluster} 的关键词')\n",
    "        axes[cluster].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cluster_wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 可视化3: 聚类统计特征对比\n",
    "def plot_cluster_statistics(data, n_clusters):\n",
    "    # 准备数据\n",
    "    stats = {}\n",
    "    numeric_cols = ['age', 'prior prison record(0/1)']\n",
    "    categorical_cols = ['race', 'gender', 'education level', 'prior occupation']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        stats[col] = [data[data['cluster_label'] == cluster][col].mean() \n",
    "                     for cluster in range(n_clusters)]\n",
    "    \n",
    "    # 创建雷达图\n",
    "    categories = list(stats.keys())\n",
    "    N = len(categories)\n",
    "    \n",
    "    # 创建角度\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # 闭合雷达图\n",
    "    \n",
    "    # 创建图表\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # 绘制每个聚类的雷达图\n",
    "    for cluster in range(n_clusters):\n",
    "        values = [stats[col][cluster] for col in categories]\n",
    "        values += values[:1]  # 闭合雷达图\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=f'聚类 {cluster}')\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # 设置坐标轴\n",
    "    ax.set_thetagrids(np.degrees(angles[:-1]), categories)\n",
    "    ax.set_title('各聚类的统计特征对比', size=15, y=1.1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    plt.savefig('cluster_statistics_radar.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 为分类特征创建柱状图\n",
    "    for col in categorical_cols:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.countplot(x=col, hue='cluster_label', data=data, palette='viridis')\n",
    "        plt.title(f'不同聚类的{col}分布')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'cluster_{col}_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# 执行可视化\n",
    "plot_clusters_pca(X, data['cluster_label'], num_clusters)\n",
    "plot_cluster_wordclouds(data, num_clusters, vectorizer)\n",
    "plot_cluster_statistics(data, num_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0865afb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     29\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mlast statement_preprocessed\u001b[39m\u001b[33m'\u001b[39m] = data[\u001b[33m'\u001b[39m\u001b[33mlast statement\u001b[39m\u001b[33m'\u001b[39m].apply(preprocess_text)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# ----------------------- BERT向量化 -----------------------\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 加载优化版中文RoBERTa模型（全词掩码+更多训练数据）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m tokenizer = \u001b[43mRobertaTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhfl/chinese-roberta-wwm-ext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m model = RobertaModel.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mhfl/chinese-roberta-wwm-ext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2025\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2022\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2023\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2278\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2276\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2277\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2278\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2279\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2280\u001b[39m     logger.info(\n\u001b[32m   2281\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2282\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2283\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\roberta\\tokenization_roberta.py:187\u001b[39m, in \u001b[36mRobertaTokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m mask_token = (\n\u001b[32m    180\u001b[39m     AddedToken(mask_token, lstrip=\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m, normalized=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[32m    183\u001b[39m )\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# these special tokens are not part of the vocab.json, let's add them in the correct order\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mself\u001b[39m.encoder = json.load(vocab_handle)\n\u001b[32m    189\u001b[39m \u001b[38;5;28mself\u001b[39m.decoder = {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encoder.items()}\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from snownlp import SnowNLP  # 提前导入SnowNLP\n",
    "\n",
    "# 文件读取地址\n",
    "file_path = r\"D:\\HuaweiMoveData\\Users\\32549\\OneDrive\\大二下\\数据科学与数据分析\\小组作业\\死刑和遗言数据.csv\"\n",
    "\n",
    "# 加载数据\n",
    "try:\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    data = pd.read_csv(file_path, encoding='gbk')\n",
    "\n",
    "# 去除缺失值\n",
    "data = data.dropna(subset=['last statement'])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = jieba.lcut(text)\n",
    "    words = [word for word in words if len(word) > 1]  # 保留双字词以上\n",
    "    return \" \".join(words)\n",
    "\n",
    "data['last statement_preprocessed'] = data['last statement'].apply(preprocess_text)\n",
    "\n",
    "# ----------------------- BERT向量化 -----------------------\n",
    "# 加载优化版中文RoBERTa模型（全词掩码+更多训练数据）\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "model = RobertaModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 批量获取句子嵌入（使用CLS token）\n",
    "def get_bert_embeddings(texts, batch_size=16):  # 减小batch_size\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            encoded = tokenizer(\n",
    "                batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            outputs = model(**encoded)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # 取CLS向量\n",
    "            embeddings.extend(cls_embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 生成BERT特征矩阵\n",
    "print(\"生成RoBERTa特征...\")\n",
    "X_bert = get_bert_embeddings(data['last statement_preprocessed'].tolist())\n",
    "\n",
    "# ----------------------- 聚类分析 -----------------------\n",
    "# 划分训练集/测试集（仅用于评估聚类稳定性）\n",
    "X_train, X_test, data_train, data_test = train_test_split(\n",
    "    X_bert, data, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 辅助函数：中文情感分析（提前定义）\n",
    "def get_sentiment_mean(cluster_data):\n",
    "    sentiments = [SnowNLP(text).sentiments for text in cluster_data['last statement_preprocessed']]\n",
    "    return np.mean(sentiments).round(2)\n",
    "\n",
    "# 寻找最优聚类数（轮廓系数法）\n",
    "silhouette_scores = []\n",
    "for n_cluster in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=n_cluster, random_state=42, n_init=10)\n",
    "    train_labels = kmeans.fit_predict(X_train)\n",
    "    test_labels = kmeans.predict(X_test)\n",
    "    train_score = silhouette_score(X_train, train_labels)\n",
    "    test_score = silhouette_score(X_test, test_labels)\n",
    "    silhouette_scores.append((n_cluster, train_score, test_score))\n",
    "    print(f\"聚类数{n_cluster}: 训练集得分{train_score:.4f}, 测试集得分{test_score:.4f}\")\n",
    "\n",
    "# 选择训练集+测试集得分最均衡的聚类数\n",
    "best_n_cluster = max(silhouette_scores, key=lambda x: (x[1]+x[2])/2)[0]\n",
    "print(f\"\\n最优聚类数: {best_n_cluster}\")\n",
    "\n",
    "# 重新训练全量数据\n",
    "kmeans = KMeans(n_clusters=best_n_cluster, random_state=42, n_init=10)\n",
    "data['cluster_label'] = kmeans.fit_predict(X_bert)\n",
    "\n",
    "# ----------------------- 聚类画像分析 -----------------------\n",
    "for cluster in range(best_n_cluster):\n",
    "    cluster_data = data[data['cluster_label'] == cluster]\n",
    "    print(f\"\\n聚类 {cluster} 分析（样本量:{len(cluster_data)}）:\")\n",
    "    print(f\"  平均年龄: {cluster_data['age'].mean():.1f} 岁\")\n",
    "    print(f\"  主要种族: {cluster_data['race'].value_counts().idxmax()}\")\n",
    "    gender_dist = cluster_data['gender'].value_counts(normalize=True).mul(100).round(1)\n",
    "    print(f\"  性别分布: {gender_dist.to_dict()}%\")  # 修正输出格式\n",
    "    print(f\"  教育程度: {cluster_data['education level'].value_counts().idxmax()}\")\n",
    "    print(f\"  犯罪特征: 监狱记录比例{cluster_data['prior prison record(0/1)'].mean():.1%}\")\n",
    "    print(f\"  情感倾向: 平均长度{cluster_data['last statement_preprocessed'].str.len().mean():.1f}字 / 平均情感得分{get_sentiment_mean(cluster_data)}\")\n",
    "    \n",
    "    # 提取高频关键词（TF-IDF top10）\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidf = TfidfVectorizer(max_features=10)\n",
    "    try:\n",
    "        tfidf_matrix = tfidf.fit_transform(cluster_data['last statement_preprocessed'])\n",
    "        keywords = tfidf.get_feature_names_out()\n",
    "        print(f\"  核心关键词: {', '.join(keywords)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  关键词提取失败: {e}\")\n",
    "    \n",
    "    # 典型遗言示例\n",
    "    print(\"  典型遗言:\")\n",
    "    for text in cluster_data['last statement'].head(2):\n",
    "        print(f\"    - {text[:80]}...\")\n",
    "    print(\"-\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
